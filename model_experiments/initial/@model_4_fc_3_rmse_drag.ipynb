{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================================================\n",
      "Grid Transformer, option: \n",
      "\tdata_path=model/data\n",
      "\tgrid_size=100\n",
      "\tgrid_resolution=2.0\n",
      "\tgrid_bound=((-50.0, 50.0), (-50.0, 50.0))\n",
      "\tsave_path=model/images/grid_100_res_2.0 initialized.\n",
      "===========================================================================\n"
     ]
    }
   ],
   "source": [
    "from src.prediction.cnn.data_transformer import BinaryImageTransformer\n",
    "\n",
    "transformer = BinaryImageTransformer(\n",
    "    data_dir=\"model/data\", grid_width=100, grid_resolution=2.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================================================\n",
      "Output matrix saved at model/data/output_matrix.npy\n",
      "===========================================================================\n",
      "===========================================================================\n",
      "Loading image matrix from model/images/grid_100_res_2.0/image_matrix.npy\n",
      "===========================================================================\n",
      "===========================================================================\n",
      "Image matrix loaded from model/images/grid_100_res_2.0/image_matrix.npy\n",
      "===========================================================================\n",
      "(750, 200, 200)\n"
     ]
    }
   ],
   "source": [
    "image_tensor = transformer.generate_images()\n",
    "print(image_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_img(images_matrix) -> None:\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    for idx, image_matrix in enumerate(images_matrix[0:10]):\n",
    "        plt.figure(figsize=(3, 3))\n",
    "        plt.imshow(\n",
    "            image_matrix,\n",
    "            cmap=\"gray\",\n",
    "            # interpolation=\"nearest\",\n",
    "        )  # Display the image using a grayscale colormap\n",
    "        plt.title(f\"Data : {idx+1}\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# plot_img(image_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "import os\n",
    "from typing import Callable, Union, Literal, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Configs and Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Setup configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Device configuration: Use GPU if available\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Data path\n",
    "IMAGE_MATRIX_PATH = transformer.image_matrix_path\n",
    "OUTPUT_MATRIX_PATH = transformer.output_matrix_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "MINI_BATCH_SIZE = 64\n",
    "TRAIN_RATIO = 0.75\n",
    "NUM_EPOCHS = 100\n",
    "EARLY_STOP_CNT = 15  # inf can be used to disable early stopping e.g) float('inf')\n",
    "LEARNING_RATE = 0.0025\n",
    "WEIGHT_DECAY = 0.0001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Load data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Define data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_set(\n",
    "    numpy_tensor_path: str, train_ratio: float, data_type: Literal[\"image\", \"tensor\"]\n",
    ") -> dict[Literal[\"train\", \"test\"], torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Loads data from a .npy file, splits it into training and testing sets,\n",
    "    and converts them to PyTorch tensors.\n",
    "\n",
    "    Args:\n",
    "        numpy_tensor_path (str): Path to the .npy file.\n",
    "        train_ratio (float): Proportion of data to use for training.\n",
    "        data_type (Literal[\"image\", \"tensor\"]): Type of data (\"image\" or \"tensor\").\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing 'train' and 'test' tensors.\n",
    "    \"\"\"\n",
    "    # Load .npy data\n",
    "    data = np.load(\n",
    "        numpy_tensor_path\n",
    "    )  # Assuming shape is (batch_size, height, width) or (batch_size, features)\n",
    "    data_len = len(data)\n",
    "    split_index = int(data_len * train_ratio)\n",
    "\n",
    "    # Split data into training and testing\n",
    "    train_set = data[:split_index]\n",
    "    test_set = data[split_index:]\n",
    "\n",
    "    # Convert NumPy arrays to PyTorch tensors\n",
    "    train_tensor = torch.from_numpy(train_set).float()\n",
    "    test_tensor = torch.from_numpy(test_set).float()\n",
    "\n",
    "    if data_type == \"image\":\n",
    "        # Add channel dimension for grayscale images: (batch_size, 1, N, N)\n",
    "        train_tensor = train_tensor.unsqueeze(1)\n",
    "        test_tensor = test_tensor.unsqueeze(1)\n",
    "\n",
    "    return {\"train\": train_tensor, \"test\": test_tensor}\n",
    "\n",
    "\n",
    "class PhysicsImageDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset for loading physics-related images and their corresponding targets.\n",
    "    Applies data augmentation (horizontal flip) to the images.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_tensor: torch.Tensor,\n",
    "        output_tensor: torch.Tensor,\n",
    "        augment: bool = False,\n",
    "        image_tensor_transformer: Union[\n",
    "            Callable[[torch.Tensor], torch.Tensor], None\n",
    "        ] = None,\n",
    "        output_tensor_transformer: Union[\n",
    "            Callable[[torch.Tensor], torch.Tensor], None\n",
    "        ] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_tensor (torch.Tensor): Tensor of images with shape (num_samples, 1, N, N).\n",
    "            output_tensor (torch.Tensor): Tensor of targets with shape (num_samples, 3).\n",
    "            augment (bool, optional): Whether to apply data augmentation. Defaults to False.\n",
    "            image_tensor_transformer (Union[Callable[[torch.Tensor], torch.Tensor], None], optional): Transformation function for the image tensor. Defaults to None.\n",
    "            output_tensor_transformer (Union[Callable[[torch.Tensor], torch.Tensor], None], optional): Transformation function for the output tensor. Defaults to None.\n",
    "        \"\"\"\n",
    "        self.image_tensor = (\n",
    "            image_tensor\n",
    "            if image_tensor_transformer is None\n",
    "            else image_tensor_transformer(image_tensor)\n",
    "        )\n",
    "        self.output_tensor = (\n",
    "            output_tensor\n",
    "            if output_tensor_transformer is None\n",
    "            else output_tensor_transformer(output_tensor)\n",
    "        )\n",
    "        self.augment = augment\n",
    "        self.origin_data_size = len(image_tensor)\n",
    "        self.transform = (\n",
    "            transforms.Compose(\n",
    "                [transforms.RandomHorizontalFlip(p=1.0)]  # Always flip when augmenting\n",
    "            )\n",
    "            if self.augment\n",
    "            else None\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        # If augmenting, double the dataset size\n",
    "        return self.origin_data_size * 2 if self.augment else self.origin_data_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.augment and idx >= self.origin_data_size:\n",
    "            # Augmented sample: flip the image horizontally\n",
    "            original_idx = idx - self.origin_data_size\n",
    "            image = self.image_tensor[original_idx]\n",
    "            image = self.transform(image)\n",
    "            target = self.output_tensor[original_idx]\n",
    "        else:\n",
    "            # Original sample\n",
    "            image = self.image_tensor[idx]\n",
    "            target = self.output_tensor[idx]\n",
    "\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Data loading process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 1124\n",
      "Number of testing samples: 376\n"
     ]
    }
   ],
   "source": [
    "# Ensure that the data paths exist\n",
    "if not os.path.exists(IMAGE_MATRIX_PATH):\n",
    "    raise FileNotFoundError(f\"Image data file not found at {IMAGE_MATRIX_PATH}\")\n",
    "if not os.path.exists(OUTPUT_MATRIX_PATH):\n",
    "    raise FileNotFoundError(f\"Output data file not found at {OUTPUT_MATRIX_PATH}\")\n",
    "\n",
    "\n",
    "def ExtractOutputProperty(\n",
    "    output_tensor: torch.Tensor, property: Literal[\"drag\", \"avg_temp\", \"max_temp\"]\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Extracts a specific property from the output tensor.\n",
    "\n",
    "    Args:\n",
    "        output_tensor (torch.Tensor): Output tensor with shape (num_samples, num_properties).\n",
    "        property_idx (int): Index of the property to extract.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Tensor containing the specified property.\n",
    "    \"\"\"\n",
    "    property_map = {\"drag\": 0, \"avg_temp\": 1, \"max_temp\": 2}\n",
    "    return output_tensor[:, property_map[property]]\n",
    "\n",
    "\n",
    "# Load image and output data\n",
    "image_data = get_data_set(\n",
    "    numpy_tensor_path=IMAGE_MATRIX_PATH,\n",
    "    train_ratio=TRAIN_RATIO,\n",
    "    data_type=\"image\",\n",
    ")\n",
    "output_data = get_data_set(\n",
    "    numpy_tensor_path=OUTPUT_MATRIX_PATH,\n",
    "    train_ratio=TRAIN_RATIO,\n",
    "    data_type=\"tensor\",\n",
    ")\n",
    "\n",
    "# Create training and testing datasets\n",
    "# Apply data augmentation (flipping) only to the training dataset\n",
    "train_dataset = PhysicsImageDataset(\n",
    "    image_tensor=image_data[\"train\"],\n",
    "    output_tensor=output_data[\"train\"],\n",
    "    augment=True,  # Enable augmentation for training\n",
    "    output_tensor_transformer=lambda x: ExtractOutputProperty(x, \"drag\"),\n",
    ")\n",
    "test_dataset = PhysicsImageDataset(\n",
    "    image_tensor=image_data[\"test\"],\n",
    "    output_tensor=output_data[\"test\"],\n",
    "    augment=True,  # No augmentation for testing\n",
    "    output_tensor_transformer=lambda x: ExtractOutputProperty(x, \"drag\"),\n",
    ")\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=MINI_BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=MINI_BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "print(f\"Number of testing samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def define_model_name(self, model_name: str) -> None:\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Formula :: NewImageSize = (PrevImageSize - KernelSize + 2PaddingSize) / Stride + 1\n",
    "\n",
    "        self.conv_layer_1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            # @Layer1 = 1x200x200 -> 32x200x200\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # @Layer2 = 32x200x200 -> 32x100x100\n",
    "        )\n",
    "\n",
    "        self.conv_layer_2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            # @Layer3 = 32x100x100 -> 64x100x100\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # @Layer4 = 64x100x100 -> 64x50x50\n",
    "        )\n",
    "\n",
    "        self.conv_layer_3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=5, stride=2, padding=1),\n",
    "            # @Layer5 = 64x50x50 -> 128x24x24\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # @Layer6 = 128x24x24 -> 128x12x12\n",
    "        )\n",
    "\n",
    "        self.conv_layer4 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            # @Layer7 = 128x12x12 -> 256x12x12\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # @Layer8 = 256x12x12 -> 256x6x6\n",
    "        )\n",
    "\n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Linear(256 * 6 * 6, 1000),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1000, 50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layer_1(x)\n",
    "        x = self.conv_layer_2(x)\n",
    "        x = self.conv_layer_3(x)\n",
    "        x = self.conv_layer4(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "\n",
    "        x = self.fc_layer(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import io\n",
    "import sys\n",
    "import uuid\n",
    "import hashlib\n",
    "\n",
    "\n",
    "def capture_print_output(func, *args, **kwargs):\n",
    "    # Create a string IO stream to capture the output\n",
    "    captured_output = io.StringIO()\n",
    "\n",
    "    # Redirect stdout to the StringIO object\n",
    "    sys.stdout = captured_output\n",
    "\n",
    "    # Execute the function\n",
    "    func(*args, **kwargs)\n",
    "\n",
    "    # Reset stdout to default\n",
    "    sys.stdout = sys.__stdout__\n",
    "\n",
    "    # Get the output from the StringIO object\n",
    "    output_value = captured_output.getvalue()\n",
    "\n",
    "    # Close the StringIO stream\n",
    "    captured_output.close()\n",
    "\n",
    "    # Return the captured output\n",
    "    return output_value\n",
    "\n",
    "\n",
    "def generate_uuid_from_seed(seed_str: str) -> uuid.UUID:\n",
    "    # Create an MD5 hash from the seed string\n",
    "    hash_object = hashlib.md5(seed_str.encode())\n",
    "\n",
    "    # Use the hash to create a UUID\n",
    "    return uuid.UUID(hash_object.hexdigest())\n",
    "\n",
    "\n",
    "def create_model_save_storage(\n",
    "    model_name: str, model_architecture: str, hyperparameter_dict: dict[str, any]\n",
    ") -> Tuple[str, str, str]:\n",
    "    \"\"\"\n",
    "    Create a dir for model experiment and record the model configuration.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): _description_\n",
    "        model_architecture (str): _description_\n",
    "        hyperparameter_dict (dict[str, any]): _description_\n",
    "\n",
    "    Returns:\n",
    "        str: model save directory path\n",
    "    \"\"\"\n",
    "    # Create a directory to store the model and its explanation\n",
    "    model_uuid = generate_uuid_from_seed(model_architecture)\n",
    "    model_dir = f\"model/models/{model_name}_{model_uuid}\"\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    hyperparameter_description_str = \"\\n\".join(\n",
    "        [f\"> {key} : {value}\" for key, value in hyperparameter_dict.items()]\n",
    "    )\n",
    "\n",
    "    model_config_path = f\"{model_dir}/config.txt\"\n",
    "    model_weight_path = f\"{model_dir}/model.pth\"\n",
    "\n",
    "    # Save the model explanation to a text file\n",
    "    with open(f\"{model_dir}/config.txt\", \"w\") as f:\n",
    "        f.write(f\"\\n{\"-\" * 100}\\n\")\n",
    "        f.write(f\"Generated at : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        f.write(\"\\n\")\n",
    "        f.write(model_name)\n",
    "        f.write(f\"\\n{\"-\" * 100}\\n\")\n",
    "        f.write(\"Hyperparameters\\n\")\n",
    "        f.write(hyperparameter_description_str)\n",
    "        f.write(f\"\\n{\"-\" * 100}\\n\")\n",
    "        f.write(\"Model Architecture\\n\")\n",
    "        f.write(model_architecture)\n",
    "\n",
    "    return model_dir, model_config_path, model_weight_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Loss function, Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE Loss\n",
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, yhat, y):\n",
    "        loss = torch.sqrt(self.mse(yhat, y) + self.eps)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = RMSELoss()\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(), lr=LEARNING_RATE\n",
    ")  # L2 regularization via weight_decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6(Optional). Learning rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import lr_scheduler\n",
    "\n",
    "STEP_SIZE = NUM_EPOCHS / 10\n",
    "# Use a learning rate scheduler that reduces LR by a factor of 0.1 every 10 epochs\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Initialize model storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "# Define the model name and save storage\n",
    "model_name = \"cnn_4_fc_3_no_batch_norm_rmse\"\n",
    "model.define_model_name(model_name)\n",
    "\n",
    "# Temporary move model to CPU for summary\n",
    "model_details = capture_print_output(\n",
    "    summary, model.to(\"cpu\"), input_size=(1, 200, 200), device=\"cpu\"\n",
    ")\n",
    "\n",
    "# Back to original device, MPS\n",
    "# Note, -1 = <batch size> is not fixed.\n",
    "model.to(device)\n",
    "\n",
    "# Create model save storage for experiment\n",
    "model_save_path, model_config_path, model_weight_path = create_model_save_storage(\n",
    "    model_name=model.model_name,\n",
    "    model_architecture=model_details,\n",
    "    hyperparameter_dict={\n",
    "        \"MINI_BATCH_SIZE\": MINI_BATCH_SIZE,\n",
    "        \"TRAIN_RATIO\": TRAIN_RATIO,\n",
    "        \"NUM_EPOCHS\": NUM_EPOCHS,\n",
    "        \"EARLY_STOP_CNT\": EARLY_STOP_CNT,\n",
    "        \"LEARNING_RATE\": LEARNING_RATE,\n",
    "        \"WEIGHT_DECAY\": WEIGHT_DECAY,\n",
    "        \"LOSS_FUNCTION\": loss_function.__str__(),\n",
    "        \"OPTIMIZER\": optimizer.__str__(),\n",
    "        # \"SCHEDULER\": scheduler.__str__(),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_training_result(\n",
    "    epoch_end: int,\n",
    "    train_loss_record: list[float],\n",
    "    val_loss_record: list[float],\n",
    "):\n",
    "    # Save the training result to a text file\n",
    "    with open(f\"{model_save_path}/training_result.json\", \"w\") as f:\n",
    "        f.write(\n",
    "            f'{{\"epoch_end\": {epoch_end}, \"train_loss\": {train_loss_record}, \"val_loss\": {val_loss_record}}}'\n",
    "        )\n",
    "\n",
    "\n",
    "def plot_loss_curve(\n",
    "    epoch_end: int, train_loss_record: list[float], val_loss_record: list[float]\n",
    "):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    epoch_end_linspace = np.linspace(0, epoch_end, len(train_loss_record))\n",
    "    plt.plot(\n",
    "        epoch_end_linspace,\n",
    "        train_loss_record,\n",
    "        label=\"Train Loss\",\n",
    "        color=\"black\",\n",
    "        linestyle=\"-\",\n",
    "    )\n",
    "    val_linspace = np.linspace(0, epoch_end, len(val_loss_record))\n",
    "    plt.plot(\n",
    "        val_linspace, val_loss_record, label=\"Val Loss\", color=\"green\", linestyle=\"--\"\n",
    "    )\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.title(f\"Cost Curve (Epochs: {epoch_end}) for model {model_name}\")\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_epoch(\n",
    "    epoch: int,\n",
    "    learning_rate: float,\n",
    "    epoch_train_loss,\n",
    "    epoch_val_loss,\n",
    "    avg_spearman_corr,\n",
    "):\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{NUM_EPOCHS} | Learning Rate: {learning_rate:.4f} | Train Loss: {epoch_train_loss:.4f} | Val Loss: {epoch_val_loss:.4f} | Avg Spearman: {avg_spearman_corr:.4f}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def train(\n",
    "    model,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    optimizer,\n",
    "    loss_function,\n",
    "    epoch_count: int,\n",
    "    early_stop_cnt: int,\n",
    "    learning_scheduler: Union[lr_scheduler.StepLR, None] = None,\n",
    ") -> Tuple[int, list[float], list[float]]:\n",
    "    print(\"Starting training...\")\n",
    "\n",
    "    best_val_loss = float(\"inf\")\n",
    "    early_stopping_counter = 0\n",
    "\n",
    "    train_loss_record = []\n",
    "    val_loss_record = []\n",
    "\n",
    "    for epoch in range(epoch_count):\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            # Move to GPU\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            predict = model(inputs)\n",
    "            # Compute loss\n",
    "            val_loss = loss_function(predict, targets)\n",
    "\n",
    "            # Backward pass\n",
    "            val_loss.backward()\n",
    "            # Optimize\n",
    "            optimizer.step()\n",
    "            # Learning rate scheduler if available\n",
    "            if learning_scheduler is not None:\n",
    "                learning_scheduler.step()\n",
    "\n",
    "            # Record the loss\n",
    "            train_loss = val_loss.item()\n",
    "            train_loss_record.append(train_loss)\n",
    "\n",
    "            running_train_loss += train_loss * inputs.size(0)  # Accumulate loss\n",
    "\n",
    "        epoch_train_loss = running_train_loss / len(train_loader.dataset)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()  # Freeze the model\n",
    "        running_val_loss = 0.0\n",
    "        val_outputs = []\n",
    "        val_targets = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in test_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                targets = targets.to(device)\n",
    "\n",
    "                predict = model(inputs)\n",
    "\n",
    "                val_loss = loss_function(predict, targets)\n",
    "                val_loss_record.append(val_loss.item())\n",
    "\n",
    "                running_val_loss += val_loss.item() * inputs.size(0)\n",
    "\n",
    "                val_outputs.append(predict.cpu().numpy())\n",
    "                val_targets.append(targets.cpu().numpy())\n",
    "\n",
    "        epoch_val_loss = running_val_loss / len(test_loader.dataset)\n",
    "\n",
    "        # Spearman correlation calc (experiment for G.A)\n",
    "        # val_outputs_np = np.concatenate(val_outputs, axis=0)\n",
    "        # val_targets_np = np.concatenate(val_targets, axis=0)\n",
    "\n",
    "        # spearman_correlation = []\n",
    "        # for i in range(val_targets_np.shape[1]):\n",
    "        #     corr, _ = spearmanr(val_targets_np[:, i], val_outputs_np[:, i])\n",
    "        #     spearman_correlation.append(corr)\n",
    "        # avg_spearman_corr = np.mean(spearman_correlation)\n",
    "\n",
    "        # Report the metrics\n",
    "        report_epoch(\n",
    "            epoch,\n",
    "            learning_rate=optimizer.param_groups[0][\"lr\"],\n",
    "            epoch_train_loss=epoch_train_loss,\n",
    "            epoch_val_loss=epoch_val_loss,\n",
    "            avg_spearman_corr=0,\n",
    "        )\n",
    "\n",
    "        # Early stopping Check\n",
    "        is_new_best = epoch_val_loss < best_val_loss\n",
    "        if is_new_best:\n",
    "            best_val_loss = epoch_val_loss\n",
    "            early_stopping_counter = 0\n",
    "            torch.save(model.state_dict(), model_weight_path)\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "            should_stop = early_stopping_counter >= early_stop_cnt\n",
    "            if should_stop:\n",
    "                print(\"Early stopping triggered.\")\n",
    "\n",
    "                return epoch, train_loss_record, val_loss_record\n",
    "\n",
    "    return epoch, train_loss_record, val_loss_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 1/100 | Learning Rate: 0.0025 | Train Loss: 0.6248 | Val Loss: 0.0316 | Avg Spearman: 0.0000\n",
      "Epoch 2/100 | Learning Rate: 0.0025 | Train Loss: 0.0381 | Val Loss: 0.0379 | Avg Spearman: 0.0000\n",
      "Epoch 3/100 | Learning Rate: 0.0025 | Train Loss: 0.0236 | Val Loss: 0.0117 | Avg Spearman: 0.0000\n",
      "Epoch 4/100 | Learning Rate: 0.0025 | Train Loss: 0.0099 | Val Loss: 0.0237 | Avg Spearman: 0.0000\n",
      "Epoch 5/100 | Learning Rate: 0.0025 | Train Loss: 0.0156 | Val Loss: 0.0209 | Avg Spearman: 0.0000\n",
      "Epoch 6/100 | Learning Rate: 0.0025 | Train Loss: 0.0078 | Val Loss: 0.0044 | Avg Spearman: 0.0000\n",
      "Epoch 7/100 | Learning Rate: 0.0025 | Train Loss: 0.0033 | Val Loss: 0.0030 | Avg Spearman: 0.0000\n",
      "Epoch 8/100 | Learning Rate: 0.0025 | Train Loss: 0.0023 | Val Loss: 0.0039 | Avg Spearman: 0.0000\n",
      "Epoch 9/100 | Learning Rate: 0.0025 | Train Loss: 0.0030 | Val Loss: 0.0025 | Avg Spearman: 0.0000\n",
      "Epoch 10/100 | Learning Rate: 0.0025 | Train Loss: 0.0036 | Val Loss: 0.0039 | Avg Spearman: 0.0000\n",
      "Epoch 11/100 | Learning Rate: 0.0025 | Train Loss: 0.0035 | Val Loss: 0.0023 | Avg Spearman: 0.0000\n",
      "Epoch 12/100 | Learning Rate: 0.0025 | Train Loss: 0.0034 | Val Loss: 0.0043 | Avg Spearman: 0.0000\n",
      "Epoch 13/100 | Learning Rate: 0.0025 | Train Loss: 0.0034 | Val Loss: 0.0027 | Avg Spearman: 0.0000\n",
      "Epoch 14/100 | Learning Rate: 0.0025 | Train Loss: 0.0032 | Val Loss: 0.0040 | Avg Spearman: 0.0000\n",
      "Epoch 15/100 | Learning Rate: 0.0025 | Train Loss: 0.0030 | Val Loss: 0.0019 | Avg Spearman: 0.0000\n",
      "Epoch 16/100 | Learning Rate: 0.0025 | Train Loss: 0.0031 | Val Loss: 0.0036 | Avg Spearman: 0.0000\n",
      "Epoch 17/100 | Learning Rate: 0.0025 | Train Loss: 0.0031 | Val Loss: 0.0018 | Avg Spearman: 0.0000\n",
      "Epoch 18/100 | Learning Rate: 0.0025 | Train Loss: 0.0029 | Val Loss: 0.0042 | Avg Spearman: 0.0000\n",
      "Epoch 19/100 | Learning Rate: 0.0025 | Train Loss: 0.0030 | Val Loss: 0.0016 | Avg Spearman: 0.0000\n",
      "Epoch 20/100 | Learning Rate: 0.0025 | Train Loss: 0.0021 | Val Loss: 0.0016 | Avg Spearman: 0.0000\n",
      "Epoch 21/100 | Learning Rate: 0.0025 | Train Loss: 0.0026 | Val Loss: 0.0017 | Avg Spearman: 0.0000\n",
      "Epoch 22/100 | Learning Rate: 0.0025 | Train Loss: 0.0021 | Val Loss: 0.0016 | Avg Spearman: 0.0000\n",
      "Epoch 23/100 | Learning Rate: 0.0025 | Train Loss: 0.0019 | Val Loss: 0.0016 | Avg Spearman: 0.0000\n",
      "Epoch 24/100 | Learning Rate: 0.0025 | Train Loss: 0.0019 | Val Loss: 0.0022 | Avg Spearman: 0.0000\n",
      "Epoch 25/100 | Learning Rate: 0.0025 | Train Loss: 0.0019 | Val Loss: 0.0023 | Avg Spearman: 0.0000\n",
      "Epoch 26/100 | Learning Rate: 0.0025 | Train Loss: 0.0018 | Val Loss: 0.0034 | Avg Spearman: 0.0000\n",
      "Epoch 27/100 | Learning Rate: 0.0025 | Train Loss: 0.0019 | Val Loss: 0.0032 | Avg Spearman: 0.0000\n",
      "Epoch 28/100 | Learning Rate: 0.0025 | Train Loss: 0.0020 | Val Loss: 0.0041 | Avg Spearman: 0.0000\n",
      "Epoch 29/100 | Learning Rate: 0.0025 | Train Loss: 0.0027 | Val Loss: 0.0026 | Avg Spearman: 0.0000\n",
      "Epoch 30/100 | Learning Rate: 0.0025 | Train Loss: 0.0027 | Val Loss: 0.0017 | Avg Spearman: 0.0000\n",
      "Epoch 31/100 | Learning Rate: 0.0025 | Train Loss: 0.0031 | Val Loss: 0.0047 | Avg Spearman: 0.0000\n",
      "Epoch 32/100 | Learning Rate: 0.0025 | Train Loss: 0.0030 | Val Loss: 0.0026 | Avg Spearman: 0.0000\n",
      "Epoch 33/100 | Learning Rate: 0.0025 | Train Loss: 0.0031 | Val Loss: 0.0035 | Avg Spearman: 0.0000\n",
      "Epoch 34/100 | Learning Rate: 0.0025 | Train Loss: 0.0028 | Val Loss: 0.0016 | Avg Spearman: 0.0000\n",
      "Epoch 35/100 | Learning Rate: 0.0025 | Train Loss: 0.0029 | Val Loss: 0.0017 | Avg Spearman: 0.0000\n",
      "Epoch 36/100 | Learning Rate: 0.0025 | Train Loss: 0.0021 | Val Loss: 0.0021 | Avg Spearman: 0.0000\n",
      "Epoch 37/100 | Learning Rate: 0.0025 | Train Loss: 0.0021 | Val Loss: 0.0023 | Avg Spearman: 0.0000\n",
      "Early stopping triggered.\n",
      "Loaded the best model based on validation loss.\n",
      "Model(\n",
      "  (conv_layer_1): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv_layer_2): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv_layer_3): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(5, 5), stride=(2, 2), padding=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv_layer4): Sequential(\n",
      "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc_layer): Sequential(\n",
      "    (0): Linear(in_features=9216, out_features=1000, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=1000, out_features=50, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "tensor([[0.2922],\n",
      "        [0.2919],\n",
      "        [0.2917],\n",
      "        [0.2917],\n",
      "        [0.2921],\n",
      "        [0.2919],\n",
      "        [0.2916],\n",
      "        [0.2920],\n",
      "        [0.2919],\n",
      "        [0.2919],\n",
      "        [0.2919],\n",
      "        [0.2920],\n",
      "        [0.2918],\n",
      "        [0.2920],\n",
      "        [0.2920],\n",
      "        [0.2917],\n",
      "        [0.2919],\n",
      "        [0.2916],\n",
      "        [0.2922],\n",
      "        [0.2919],\n",
      "        [0.2921],\n",
      "        [0.2919],\n",
      "        [0.2920],\n",
      "        [0.2922],\n",
      "        [0.2919],\n",
      "        [0.2921],\n",
      "        [0.2919],\n",
      "        [0.2919],\n",
      "        [0.2917],\n",
      "        [0.2917],\n",
      "        [0.2920],\n",
      "        [0.2921],\n",
      "        [0.2919],\n",
      "        [0.2919],\n",
      "        [0.2922],\n",
      "        [0.2917],\n",
      "        [0.2922],\n",
      "        [0.2921],\n",
      "        [0.2921],\n",
      "        [0.2921],\n",
      "        [0.2914],\n",
      "        [0.2920],\n",
      "        [0.2917],\n",
      "        [0.2918],\n",
      "        [0.2920],\n",
      "        [0.2919],\n",
      "        [0.2921],\n",
      "        [0.2919],\n",
      "        [0.2917],\n",
      "        [0.2919],\n",
      "        [0.2917],\n",
      "        [0.2923],\n",
      "        [0.2921],\n",
      "        [0.2918],\n",
      "        [0.2919],\n",
      "        [0.2917],\n",
      "        [0.2922],\n",
      "        [0.2920],\n",
      "        [0.2918],\n",
      "        [0.2919],\n",
      "        [0.2918],\n",
      "        [0.2919],\n",
      "        [0.2917],\n",
      "        [0.2919]], device='mps:0')\n",
      "tensor([[0.2917],\n",
      "        [0.2916],\n",
      "        [0.2919],\n",
      "        [0.2917],\n",
      "        [0.2917],\n",
      "        [0.2920],\n",
      "        [0.2920],\n",
      "        [0.2919],\n",
      "        [0.2920],\n",
      "        [0.2916],\n",
      "        [0.2920],\n",
      "        [0.2919],\n",
      "        [0.2918],\n",
      "        [0.2916],\n",
      "        [0.2919],\n",
      "        [0.2919],\n",
      "        [0.2916],\n",
      "        [0.2914],\n",
      "        [0.2918],\n",
      "        [0.2919],\n",
      "        [0.2915],\n",
      "        [0.2917],\n",
      "        [0.2919],\n",
      "        [0.2919],\n",
      "        [0.2919],\n",
      "        [0.2918],\n",
      "        [0.2918],\n",
      "        [0.2917],\n",
      "        [0.2917],\n",
      "        [0.2919],\n",
      "        [0.2917],\n",
      "        [0.2916],\n",
      "        [0.2919],\n",
      "        [0.2916],\n",
      "        [0.2919],\n",
      "        [0.2920],\n",
      "        [0.2916],\n",
      "        [0.2913],\n",
      "        [0.2916],\n",
      "        [0.2918],\n",
      "        [0.2919],\n",
      "        [0.2919],\n",
      "        [0.2915],\n",
      "        [0.2917],\n",
      "        [0.2918],\n",
      "        [0.2917],\n",
      "        [0.2915],\n",
      "        [0.2916],\n",
      "        [0.2918],\n",
      "        [0.2919],\n",
      "        [0.2916],\n",
      "        [0.2918],\n",
      "        [0.2916],\n",
      "        [0.2917],\n",
      "        [0.2917],\n",
      "        [0.2919],\n",
      "        [0.2916],\n",
      "        [0.2917],\n",
      "        [0.2918],\n",
      "        [0.2916],\n",
      "        [0.2914],\n",
      "        [0.2916],\n",
      "        [0.2917],\n",
      "        [0.2913]], device='mps:0')\n",
      "tensor([[0.2916],\n",
      "        [0.2916],\n",
      "        [0.2915],\n",
      "        [0.2915],\n",
      "        [0.2917],\n",
      "        [0.2916],\n",
      "        [0.2914],\n",
      "        [0.2918],\n",
      "        [0.2916],\n",
      "        [0.2917],\n",
      "        [0.2915],\n",
      "        [0.2916],\n",
      "        [0.2917],\n",
      "        [0.2917],\n",
      "        [0.2915],\n",
      "        [0.2913],\n",
      "        [0.2915],\n",
      "        [0.2915],\n",
      "        [0.2916],\n",
      "        [0.2916],\n",
      "        [0.2915],\n",
      "        [0.2915],\n",
      "        [0.2915],\n",
      "        [0.2915],\n",
      "        [0.2915],\n",
      "        [0.2914],\n",
      "        [0.2914],\n",
      "        [0.2913],\n",
      "        [0.2914],\n",
      "        [0.2914],\n",
      "        [0.2915],\n",
      "        [0.2914],\n",
      "        [0.2914],\n",
      "        [0.2914],\n",
      "        [0.2913],\n",
      "        [0.2914],\n",
      "        [0.2914],\n",
      "        [0.2913],\n",
      "        [0.2914],\n",
      "        [0.2911],\n",
      "        [0.2914],\n",
      "        [0.2913],\n",
      "        [0.2912],\n",
      "        [0.2912],\n",
      "        [0.2912],\n",
      "        [0.2912],\n",
      "        [0.2911],\n",
      "        [0.2911],\n",
      "        [0.2911],\n",
      "        [0.2911],\n",
      "        [0.2911],\n",
      "        [0.2919],\n",
      "        [0.2919],\n",
      "        [0.2920],\n",
      "        [0.2915],\n",
      "        [0.2915],\n",
      "        [0.2916],\n",
      "        [0.2915],\n",
      "        [0.2913],\n",
      "        [0.2920],\n",
      "        [0.2922],\n",
      "        [0.2921],\n",
      "        [0.2919],\n",
      "        [0.2918]], device='mps:0')\n",
      "tensor([[0.2922],\n",
      "        [0.2921],\n",
      "        [0.2917],\n",
      "        [0.2921],\n",
      "        [0.2919],\n",
      "        [0.2920],\n",
      "        [0.2919],\n",
      "        [0.2920],\n",
      "        [0.2919],\n",
      "        [0.2921],\n",
      "        [0.2920],\n",
      "        [0.2919],\n",
      "        [0.2918],\n",
      "        [0.2916],\n",
      "        [0.2921],\n",
      "        [0.2921],\n",
      "        [0.2921],\n",
      "        [0.2920],\n",
      "        [0.2921],\n",
      "        [0.2922],\n",
      "        [0.2920],\n",
      "        [0.2922],\n",
      "        [0.2920],\n",
      "        [0.2919],\n",
      "        [0.2917],\n",
      "        [0.2919],\n",
      "        [0.2920],\n",
      "        [0.2922],\n",
      "        [0.2921],\n",
      "        [0.2919],\n",
      "        [0.2922],\n",
      "        [0.2918],\n",
      "        [0.2922],\n",
      "        [0.2920],\n",
      "        [0.2920],\n",
      "        [0.2921],\n",
      "        [0.2914],\n",
      "        [0.2920],\n",
      "        [0.2918],\n",
      "        [0.2918],\n",
      "        [0.2920],\n",
      "        [0.2921],\n",
      "        [0.2921],\n",
      "        [0.2920],\n",
      "        [0.2917],\n",
      "        [0.2918],\n",
      "        [0.2920],\n",
      "        [0.2923],\n",
      "        [0.2921],\n",
      "        [0.2921],\n",
      "        [0.2920],\n",
      "        [0.2918],\n",
      "        [0.2923],\n",
      "        [0.2921],\n",
      "        [0.2918],\n",
      "        [0.2920],\n",
      "        [0.2919],\n",
      "        [0.2919],\n",
      "        [0.2916],\n",
      "        [0.2920],\n",
      "        [0.2919],\n",
      "        [0.2918],\n",
      "        [0.2920],\n",
      "        [0.2917]], device='mps:0')\n",
      "tensor([[0.2917],\n",
      "        [0.2919],\n",
      "        [0.2920],\n",
      "        [0.2919],\n",
      "        [0.2920],\n",
      "        [0.2917],\n",
      "        [0.2920],\n",
      "        [0.2918],\n",
      "        [0.2917],\n",
      "        [0.2918],\n",
      "        [0.2921],\n",
      "        [0.2920],\n",
      "        [0.2917],\n",
      "        [0.2915],\n",
      "        [0.2918],\n",
      "        [0.2919],\n",
      "        [0.2916],\n",
      "        [0.2919],\n",
      "        [0.2918],\n",
      "        [0.2919],\n",
      "        [0.2919],\n",
      "        [0.2918],\n",
      "        [0.2918],\n",
      "        [0.2918],\n",
      "        [0.2918],\n",
      "        [0.2919],\n",
      "        [0.2918],\n",
      "        [0.2917],\n",
      "        [0.2920],\n",
      "        [0.2917],\n",
      "        [0.2919],\n",
      "        [0.2919],\n",
      "        [0.2916],\n",
      "        [0.2914],\n",
      "        [0.2917],\n",
      "        [0.2918],\n",
      "        [0.2919],\n",
      "        [0.2918],\n",
      "        [0.2916],\n",
      "        [0.2917],\n",
      "        [0.2918],\n",
      "        [0.2917],\n",
      "        [0.2915],\n",
      "        [0.2917],\n",
      "        [0.2919],\n",
      "        [0.2919],\n",
      "        [0.2917],\n",
      "        [0.2918],\n",
      "        [0.2916],\n",
      "        [0.2918],\n",
      "        [0.2916],\n",
      "        [0.2920],\n",
      "        [0.2918],\n",
      "        [0.2917],\n",
      "        [0.2917],\n",
      "        [0.2917],\n",
      "        [0.2915],\n",
      "        [0.2916],\n",
      "        [0.2917],\n",
      "        [0.2914],\n",
      "        [0.2916],\n",
      "        [0.2917],\n",
      "        [0.2916],\n",
      "        [0.2915]], device='mps:0')\n",
      "tensor([[0.2917],\n",
      "        [0.2917],\n",
      "        [0.2914],\n",
      "        [0.2918],\n",
      "        [0.2917],\n",
      "        [0.2917],\n",
      "        [0.2915],\n",
      "        [0.2917],\n",
      "        [0.2917],\n",
      "        [0.2917],\n",
      "        [0.2916],\n",
      "        [0.2914],\n",
      "        [0.2916],\n",
      "        [0.2915],\n",
      "        [0.2917],\n",
      "        [0.2916],\n",
      "        [0.2915],\n",
      "        [0.2915],\n",
      "        [0.2915],\n",
      "        [0.2915],\n",
      "        [0.2916],\n",
      "        [0.2914],\n",
      "        [0.2914],\n",
      "        [0.2914],\n",
      "        [0.2914],\n",
      "        [0.2914],\n",
      "        [0.2915],\n",
      "        [0.2914],\n",
      "        [0.2914],\n",
      "        [0.2914],\n",
      "        [0.2913],\n",
      "        [0.2915],\n",
      "        [0.2914],\n",
      "        [0.2913],\n",
      "        [0.2914],\n",
      "        [0.2911],\n",
      "        [0.2915],\n",
      "        [0.2914],\n",
      "        [0.2912],\n",
      "        [0.2912],\n",
      "        [0.2913],\n",
      "        [0.2912],\n",
      "        [0.2911],\n",
      "        [0.2911],\n",
      "        [0.2911],\n",
      "        [0.2911],\n",
      "        [0.2911],\n",
      "        [0.2921],\n",
      "        [0.2921],\n",
      "        [0.2920],\n",
      "        [0.2916],\n",
      "        [0.2915],\n",
      "        [0.2918],\n",
      "        [0.2915],\n",
      "        [0.2913],\n",
      "        [0.2922]], device='mps:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/june/Documents/project/capstone_1/.venv/lib/python3.12/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/june/Documents/project/capstone_1/.venv/lib/python3.12/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([36])) that is different to the input size (torch.Size([36, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/june/Documents/project/capstone_1/.venv/lib/python3.12/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([56])) that is different to the input size (torch.Size([56, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "final_epoch, train_loss_record, val_loss_record = train(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    optimizer=optimizer,\n",
    "    loss_function=loss_function,\n",
    "    epoch_count=NUM_EPOCHS,\n",
    "    early_stop_cnt=EARLY_STOP_CNT,\n",
    "    # learning_scheduler=scheduler,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "save_model_training_result(final_epoch, train_loss_record, val_loss_record)\n",
    "\n",
    "plot_loss_curve(\n",
    "    epoch_end=final_epoch,\n",
    "    train_loss_record=train_loss_record,\n",
    "    val_loss_record=val_loss_record,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Evaluation(Validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/x6/l7qs54m50blbxn2wj8_dbm8m0000gn/T/ipykernel_16007/2951859011.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_weight_path))\n"
     ]
    }
   ],
   "source": [
    "def load_best_model(model) -> None:\n",
    "    model.load_state_dict(torch.load(model_weight_path))\n",
    "    model.eval()\n",
    "    print(\"Loaded the best model based on validation loss.\")\n",
    "\n",
    "\n",
    "model = Model().to(device)\n",
    "load_best_model(model)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Inference by test-set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/june/Documents/project/capstone_1/.venv/lib/python3.12/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/june/Documents/project/capstone_1/.venv/lib/python3.12/site-packages/torch/nn/modules/loss.py:538: UserWarning: Using a target size (torch.Size([56])) that is different to the input size (torch.Size([56, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m val_targets_np \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(val_targets, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     25\u001b[0m spearman_correlation \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[43mval_targets_np\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m):\n\u001b[1;32m     27\u001b[0m     corr, _ \u001b[38;5;241m=\u001b[39m spearmanr(val_targets_np[:, i], val_outputs_np[:, i])\n\u001b[1;32m     28\u001b[0m     spearman_correlation\u001b[38;5;241m.\u001b[39mappend(corr)\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "running_val_loss = 0.0\n",
    "val_outputs = []\n",
    "val_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        predict = model(inputs)\n",
    "        print(predict)\n",
    "        val_loss = loss_function(predict, targets)\n",
    "\n",
    "        running_val_loss += val_loss.item() * inputs.size(0)\n",
    "\n",
    "        val_outputs.append(predict.cpu().numpy())\n",
    "        val_targets.append(targets.cpu().numpy())\n",
    "\n",
    "final_val_loss = running_val_loss / len(test_loader.dataset)\n",
    "\n",
    "# Concatenate all outputs and targets for Spearman correlation\n",
    "val_outputs_np = np.concatenate(val_outputs, axis=0)\n",
    "val_targets_np = np.concatenate(val_targets, axis=0)\n",
    "\n",
    "spearman_correlation = []\n",
    "for i in range(val_targets_np.shape[1]):\n",
    "    corr, _ = spearmanr(val_targets_np[:, i], val_outputs_np[:, i])\n",
    "    spearman_correlation.append(corr)\n",
    "final_avg_spearman_corr = np.mean(spearman_correlation)\n",
    "\n",
    "print(\n",
    "    f\"Final Validation Loss: {final_val_loss:.4f} | Final Avg Spearman: {final_avg_spearman_corr:.4f}\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
